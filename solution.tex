%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)


\usepackage{graphicx}
\graphicspath{ {./imgs/} }

\usepackage{float}

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text
\setlength{\parskip}{10pt plus 1pt minus 1pt}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{University if Helsinki, Department of Computer Science} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Markov Modeling and Bayesian Learning \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Han Xiao} % Your namea

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{Problem 1}

\lipsum[2] % Dummy text

Sequence generation:

For some given sequence length $T$, the following configuraiton is made:

\begin {itemize}
  \item Each sequence is generated according to some {\em random} state transition probability table $P$.
  \item Each entry in table $P$ are equal
  \item The starting probabilities for each bigram are equal (0.25).
\end {itemize}


Then {\em 1000} sequences was randomly generated, each of which is used to estimate the parameters by Maximum Likelihood estimation and Bayesian estimation.

Denote the estimated parameters as $P'_{ijk}$ and the true parameters as $P_{ijk}$, the error rate for each estimated paraemter is calculated as:

\[ E_{ijk} = \frac{|P'_{ijk} - P_{ijk}|} {P_{ijk}} \]

And the total error rate of the estimated paramters is calculated as the mean of all the individual error rates:

\[E_{total} = \frac {1} {n} \sum\limits_{ijk} \frac{|P'_{ijk} - P_{ijk}| }{ P_{ijk}}\]

where $n$ is the number of paramters, which is $64$ in this case.

The resuling plot is like: 

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{mle-be-estimation}
  \caption{Comparison between MLE and BE for various sequence lengths}
\end{figure}

%how to set matlab plot size
%set(0, 'DefaultFigurePosition', [ left bottom width height ]);


%----------------------------------------------------------------------------------------
%	PROBLEM 2
%----------------------------------------------------------------------------------------

\section{Bayes Factor}

\subsection {Theory}

We denote $M_0$ as the zero order Markov process, also known as the multinomial process, $M_1$ as the first order Markov process and $M_2$ as the second order Markov process and $x$ as the observed sequence. Then the Bayes factor between $M_1$ and $M_2$ and between $M_0$ and $M_1$ are defined as:

\begin {align*}
  B_{01} &= \frac {p (x| M_0)} {p(x | M_0} \\
  B_{12} &= \frac {p (x| M_1)} {p(x | M_2)}
\end {align*}

For the Markov 0 model, the predictive probability 

\begin {equation}
p (x| M_0) = \int_{\Theta} \theta_{A}^{n_{A}} \theta_{C}^{n_{C}} \theta_{G}^{n_{G}} \theta_{T}^{n_{T}} d\Theta
\end {equation}

in which $\theta_A, \theta_C, \theta_G, \theta_T$ are the multinomial process parameters, which sum up to 1. $n_A, n_C, n_G, n_T$ are the number of occurences of ${A, C, G, T}$ respectively. In addition $\Theta = (\theta_A, \theta_C, \theta_G, \theta_T)$.
\\\\
Similarly, the first order Markov chain case involves four independent multinimial processes, in which each process is preceded by one of the values. The predictive probability in this case is:

\begin {align*}
p (x| M_1) &= \int_{\Theta_A} \theta_{AA}^{n_{AA}} \theta_{AC}^{n_{AC}} \theta_{AG}^{n_{AG}} \theta_{AT}^{n_{AT}} d\Theta_A \times\\
&= \int_{\Theta_C} \theta_{CA}^{n_{CA}} \theta_{CC}^{n_{CC}} \theta_{CG}^{n_{CG}} \theta_{CT}^{n_{CT}} d\Theta_C \times \\
&= \int_{\Theta_G} \theta_{GA}^{n_{GA}} \theta_{GC}^{n_{GC}} \theta_{GG}^{n_{GG}} \theta_{GT}^{n_{GT}} d\Theta_G \times \\
&= \int_{\Theta_T} \theta_{TA}^{n_{TA}} \theta_{TC}^{n_{GC}} \theta_{TG}^{n_{TG}} \theta_{TT}^{n_{TT}} d\Theta_T \\
\end {align*}

in which, $n_{i,j}$, where $i,j \in {A,C,G,T}$, denotes the number of occurences of $(i,j)$ pairs in the sequence. Similarly, $\theta_{i,j}$ denotes the probability for $j$ to occur in case of being preceded by $i$. $\Theta_i$ denotes the parameter set $(\theta_{i,A}, \theta_{i,C}, \theta_{i,G}, \theta_{i,T})$.
\\\\
Similarly, the second order Markov chain is extended to 16 independent multinomial process. The predictive probability is:

\begin {equation}
  p (x| M_2) = \prod_{i,j} \int_{\Theta_{ij}} \theta_{i,j,A}^{n_{i,j,A}} \theta_{i,j,C}^{n_{i,j,C}} \theta_{i,j,G}^{n_{i,j,G}} \theta_{i,j,T}^{n_{i,j,T}} d\Theta_{i,j} 
\end {equation}  

Assuming a Dirichlet distribution as the prior for the multinomial process, the integral has analytical result. 
\\\\
For the Markov 0 model, 
\begin {equation}
p (x| M_0) = \frac{\Gamma (\alpha_A + \alpha_C + \alpha_G + \alpha_T) \prod\limits_{m \in {A,C,G,T}} \Gamma (\alpha_m + n_m)} {\prod\limits_{m \in {A,C,G,T}} \Gamma (\alpha_m) \Gamma (\sum\limits_{m \in {A,C,G,T}}\alpha_m + \sum\limits_{m \in {A,C,G,T}}n_m)}
\end {equation}  

For the Markov 1 model

\begin {equation}
p (x| M_1) = \prod\limits_{i \in {A,C,G,T}} \frac{\Gamma (\alpha_{iA} + \alpha_{iC} + \alpha_{iG} + \alpha_iT) \prod\limits_{m \in {A,C,G,T}} \Gamma (\alpha_{im} + n_{im})} {\prod\limits_{m \in {A,C,G,T}} \Gamma (\alpha_{im}) \Gamma (\sum\limits_{m \in {A,C,G,T}}\alpha_{im} + \sum\limits_{m \in {A,C,G,T}}n_{im})}
\end {equation}  

For the Markov 2 model

\begin {equation}
p (x| M_1) = \prod\limits_{i,j \in {A,C,G,T}} \frac{\Gamma (\alpha_{ijA} + \alpha_{ijC} + \alpha_{ijG} + \alpha_ijT) \prod\limits_{m \in {A,C,G,T}} \Gamma (\alpha_{ijm} + n_{ijm})} {\prod\limits_{m \in {A,C,G,T}} \Gamma (\alpha_{ijm}) \Gamma (\sum\limits_{m \in {A,C,G,T}}\alpha_{ijm} + \sum\limits_{m \in {A,C,G,T}}n_{ijm})}
\end {equation}  

\subsection {Experiment Setup}

I generated 1000 sequences, each of which is generated based on randomly generated state transition table, in which each entry in the table are uniformly distribued (and then normalized). In other word, two levels of randomness are involved. First level is associated with the random nature of the generation of gene sequence, which is essentially a Markov random process. Second level of randomness is associated with the randomness of Markov state transition probability table. 
\\\\ 
The purpose of the extra level of randomness is to avoid using a fixed or a fixed set of Markov random process, thus enable us to observe Bayes Factor in the general case.

\subsection {Pitfall of Prior Setup}
One thing to notice is the setting of prior parameter. For the Markov 0 case, it is tempting to set the prior as $Beta (1, 1, 1, 1)$ and for the markov 1 case, four priors all be the same, $Beta (1, 1, 1, 1)$. However, there is one problem here in terms of both interpretation and theory. In tmerms of interpretation, the prior $Beta (1,1,1,1)$  resembles that we have some statistics in which each type of values appear once prior to our experiment. Then unfairness arises. For the Markov 0 case, we have 4 such values, while for Markov 1 case, with 4 priors, we have 16 such values ``beforehand'', which is not reasonable. 
\\\\
To make it fair, we should ensure that the number of prior ``observed'' values we have are equal for all models, for example, 64 values, which is also the parameter number for Markov 2. Thus,I set the prior distribution for Markov 0 be a Dirichlet with hyperparamter $(16, 16, 16, 16)$, while the prior distribution for Markov 1 be four independent Dirichlet, with hyperparameter $(4,4,4,4)$ and for the Markov 2 case 16 independent Dirichelet with hyperparameter $(1,1,1,1)$.


\subsection {Experiment Result}

{\bf Bayes factor histograms}
\\\\
Using the 500 values in the sequence, 1000 Bayes factor in terms of Markov 0 vs Markov 1 and Markov 1 vs Markov 2 for each of the sequences are generated. The histograms are as followed. We can see that Bayes Factor favors Markov 2 over Markov 1 and Markov 1 over Markov 0.
 
\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{bf-hist}
  \caption{Histogram of Bayes Factor beween Markov (0){\em vs} Markov (1) (Left) and Markov (1) {\em vs} Markov (2) (Right)}
\end{figure}

The mean value for $B_{01}$ is 0.0012 and mean value for $B_{12}$ is 2.3740e-21. The model that Bayes factor supports is indeed the real generating model.
\\\\
{\bf Bayes factor vs number of values}
\\\\
The relationship between Bayes factor and the number of values used in calculation is plotted as followed:

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{bf-nval}
  \caption{Bayes factor vs number of values}
\end{figure}

%----------------------------------------------------------------------------------------
%	PROBLEM 4
%----------------------------------------------------------------------------------------

\section{Problem 4}
\subsection {Phenomenon}
I fix $p$ and vary the error parameter $\varepsilon$ from 0 to 1 at a step size of 0.05. Then for each $\varepsilon$, 500 HMM sequences are generated randomly by $P$ and $E$.

The error of the parameter estimation $P^{'}$ is defined as the mean of the element-wise difference between $P$ and $P^{'}$ 

\[ \mathrm{Error} (P^{'}, P) = \frac {1} {4} \sum_{i} \sum_{j} \frac{|P^{'}_{i,j} - P_{i,j}|}{P_{i,j}} \]

The error mean for $p \in \{0.2, 0.5, 0.8\}$ are given below:

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{{noisyhmm-errorrate-p=0.200000}.png}
  \caption {ML parameter estimation error when $p$=0.2}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{{noisyhmm-errorrate-p=0.500000}.png}
  \caption {ML parameter estimation error when $p$=0.5}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{{noisyhmm-errorrate-p=0.800000}.png}
  \caption {ML parameter estimation error when $p$=0.8}
\end{figure}

\subsection {Explanation}

There are two remarkable oberservations.

\begin{itemize}
\item for $p=0.2$ and $p=0.8$, the error mean first increases. After reaching the peak at about 0.8, it starts to fall.
\item while the plot for $p=0.2$ and $p=0.8$ are very similar, the plot for $p=0.5$ is not the case, it is flat!
\end {itemize}

For the {\em first observation}:\newline

Denote the state set as $\{S_1, S_2\}$ and the observation set as $\{O_1, O_2\}$. \newline

For the first observation, it is intuitive to come up that lower error mean results from lower $\varepsilon$. Because the observations reflect the true state transition more or less accurately. 

However, it might be counterintuitive for large $\varepsilon$. Here are some explanation for this counter-intuition. \newline

Let's assume $\varepsilon=1$, in which case each observation is flipped from its true underlying state. That is $S_1$ generates $O_2$ and $S_2$ generates $O_1$. 

We denote the count of certain observations as $C (O_1)$ or $C (O_1, O_2)$ for the count of $(O_1, O_2)$ pairs. Then $C (O_1, O_1) = C (S_2, S_2)$, $C (O_2, O_2) = C (S_1, S_1)$ , $C (O_2, O_1) = C (S_1, S_2)$ and $C (O_1, O_2) = C (S_2, S_1)$ 

As there is only one parameter $p$ for the HMM state transition process, $C(S_1,S_1) \approx C(S_2,S_2)$ should be roughly equal if sample size is large enough. Similarly, $C(S_1, S_2) \approx C(S_2, S_1)$. 

Also assuming a uniform initial state distribution, $C (S_1) \approx C (S_2)$. \newline

As a result, $C (O_1, O_1) \approx C (O_2, O_2)$ and $C (O_2, O_1) \approx C (O_1, O_2)$ and $C (O_1) \approx C (O_2)$. 

As a illustration, we estimate the probability from $S_1$ to $S_2$:

\begin {align*}
  P (S_1, S_2) &= \frac {C (S_2)} {C (S_1, S_2)} \\
  &= \frac {C (O_1)} {C (O_2, O_1)} \\
  &\approx \frac {C (O_2)} {C (O_1, O_2)}\\
  &=P (O_1, O_2)
\end {align*}

Therefore, the error mean for large $\varepsilon$ is quite close to that of small $\varepsilon$.\newline

For the {\em second observation}:\newline

When $p=0.5$, the state transition process is like tossing a fair coin again and again.

The low error mean for both small and great $p$ can be explained like above. In case $\varepsilon=0.5$, flipping of the observation is like another process of tossing fair coin.

Though certain observations may be flipped, the cases in which $S_1$ emits $O_2$ are compansated by the fact that roughly the same amount of emission from $S_2$ to $O_1$, thus the sufficient statistics does not change much.

As a conclusion, the error mean for $p=0.5$ does not vary much from that of large or small $p$ values.

%----------------------------------------------------------------------------------------
%	PROBLEM 5
%----------------------------------------------------------------------------------------
\section{Problem 5}
\subsection {Choice of HMM parameter}

The state transition probability matrix is set to:

\begin{table}[H]
\caption{State transition probability matrix}
\centering
\begin{tabular}{c|ccc}
  & CpG rich & CpG poor  \\ \hline
  CpG rich & 0.99 & 0.01 \\
  CpG poor & 0.01 & 0.99
\end {tabular}
\end {table}


The emission probability matrix is set to:

\begin{table}[H]
\caption{Emission probability matrix}
\centering
\begin{tabular}{c|cccc}
  & A & C & G & T  \\ \hline
  CpG rich & 0.05 & 0.49 & 0.49 & 0.05 \\
  CpG poor & 0.49 & 0.05 & 0.05 & 0.49
\end {tabular}
\end {table}

In this way, we can ensure that state is very unlikely to transfer to the other counterpart and CpG rich state is much likely to produces $C$ and $G$, while CpG poor state is much likely to produces $A$ and $T$
\subsection {Visualization}

100 sequences each of length 100 using the preivously stated model are randomly generated. To achieve more natural visual contrast between the state sequence and emission sequence, the emitted symbol set are transformed in the following way, $G$ and $C$ are combined into one state, CpG rich, while $A$ and $T$ are combined into CpG poor. Thus, emission sequences are converted into the same type as state sequence, which involves only two states.

The visualization for both sequences is as followed:

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{gene_viz}
  \caption {Visualization for 100 transformed emission sequences(left) and the underlying state sequences(right).}
  \label {fig:5.gene_viz}
\end{figure}

As we can see, state sequences are ``tidier'' than the emission sequence in the sense that there are fewer times of state flipping.

\subsection {Modeling as oridinary Markov chain}

In case of modeling this process using ordinary first order Markov chain, the state space becomes ${\text{CpG rich}, \text{CpG poor}}$, the problem of paramter estimation boils down to estimate the state transition probabilites.

ML estimation is chosen as the  parameter estimation method. 

Before estimation, we reassign the emission symbols in the following way, $C$ and $G$ are treated as state CpG rich and $A$ and $T$ are treated as state CpG poor.  This is done to fit the data into the proposed model. No smoothing technique is used here.

The result is:

\begin{table}[H]
\caption{Estimated state transition probability matrix}
\centering
\begin{tabular}{c|ccc}
  & CpG rich & CpG poor  \\ \hline
  CpG rich & 0.8669 &   0.1331 \\
  CpG poor & 0.2678 &   0.7322
\end {tabular}
\end {table}

As we can see, compared to the generating parameter, the estimated result is ``smoothed out'' to some extent, in the sense that the probabilitis are centered, compared to the true values. For example, probability from CpG rich to CpG rich is 0.8669, which is less than the true value, 0.99, while from CpG rich to CpG poor yields probability 0.1331, which is larger than 0.01.

The cause for this phenomenon is due to the randomness in the emission process. There is some possibility for CpG rich state to produce symbols $A$ and $T$, though such probability is low. One direct consequence is the interspersing constrasting subsequences (e.g, the little green spots in the left image in \ref{fig:5.gene_viz}) in the emisbsion sequences. Those contrasting short subsequences increases occurences of CpG rich transiting to CpG poor and CpG poor transiting to CpG rich. Thus, they serve as the ``bridge'' that connects the underlying two states and make them seem to transit to each other more often if the ordiinary Markov model is considered.

%------------------------------------------------

\subsection{Example of list (3*itemize)}
\begin{itemize}
	\item First item in a list 
		\begin{itemize}
		\item First item in a list 
			\begin{itemize}
			\item First item in a list 
			\item Second item in a list 
			\end{itemize}
		\item Second item in a list 
		\end{itemize}
	\item Second item in a list 
\end{itemize}

%------------------------------------------------

\subsection{Example of list (enumerate)}
\begin{enumerate}
\item First item in a list 
\item Second item in a list 
\item Third item in a list
\end{enumerate}

%----------------------------------------------------------------------------------------

\end{document}
