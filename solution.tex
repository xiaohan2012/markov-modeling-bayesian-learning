%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)


\usepackage{graphicx}
\graphicspath{ {./imgs/} }

\usepackage{float}

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text
\setlength{\parskip}{10pt plus 1pt minus 1pt}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{University of Helsinki, Department of Computer Science} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Markov Modeling and Bayesian Learning \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Han Xiao} % Your namea

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{Problem 1}

\subsection{Sequence generation}

For given sequence length $T$, which can be arbitrary, the following configuration is made to generate sequences randomly:

\begin {itemize}
  \item Each sequence is generated according to some {\em random} state transition probability table $P$.
  \item Each probability entry in table $P$ is  equal to each other
  \item The starting probabilities for each bigram are equal (0.25)
\end {itemize}

Then 1000 sequences are randomly generated in order to reduce the randomness of the final result. 

\subsection{Error measurement}
Denote the estimated parameters as $P'_{ijk}$ and the true parameters as $P_{ijk}$, the error rate for each estimated parameter is calculated as:

\[ E_{ijk} = \frac{|P'_{ijk} - P_{ijk}|} {P_{ijk}} \]

And the total error rate of the estimated parameter's is calculated as the mean of all the individual error rates:

\[E_{total} = \frac {1} {n} \sum\limits_{ijk} \frac{|P'_{ijk} - P_{ijk}| }{ P_{ijk}}\]

where $n$ is the number of parameters, which is $64$ in this case.

\subsection{Estimation}
Second order Markov model is used in estimation. For maximum likelihood estimation, the estimated result is acquired by dividing the sufficient statistics of trigrams by that of the corresponding bigrams, while for Bayesian estimation, conjugate prior equal to Dirichlet distribution with hyperparameters $\lambda=(1, \dots, 1)$ is used.

\subsection{Simulation result}
The following simulation is done. Sequence lengths, 100, 200, 300, 400, 500 are considered and 1000 sequences for each length are generated randomly and independently, ML estimation and Bayesian estimation are performed and the corresponding error for each method are calculated.

The final plot for the error rate of both methods in case of different length is as followed:

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{mle-be-estimation}
  \caption{Comparison between Maximum Likelihood estimation and Bayes-ion estimation under sequence length 100, 200, 300, 400, 500}
\end{figure}

As we can see, as the sequence length increases, the estimation becomes more accurate. This result is expected.

%----------------------------------------------------------------------------------------
%	PROBLEM 2
%----------------------------------------------------------------------------------------

\section{Bayes Factor}

\subsection {Theory}

We denote $M_0$ as the zero order Markov process, also known as the multinomial process, $M_1$ as the first order Markov process and $M_2$ as the second order Markov process and $x$ as the observed sequence. Then the Bayes factor between $M_1$ and $M_2$ and between $M_0$ and $M_1$ are defined as:

\begin {align*}
  B_{01} &= \frac {p (x| M_0)} {p(x | M_0} \\
  B_{12} &= \frac {p (x| M_1)} {p(x | M_2)}
\end {align*}

For the Markov 0 model, the predictive probability 

\begin {equation}
p (x| M_0) = \int_{\Theta} \theta_{A}^{n_{A}} \theta_{C}^{n_{C}} \theta_{G}^{n_{G}} \theta_{T}^{n_{T}} d\Theta
\end {equation}

in which $\theta_A, \theta_C, \theta_G, \theta_T$ are the multinomial process parameters, which sum up to 1. $n_A, n_C, n_G, n_T$ are the number of occurrences of ${A, C, G, T}$ respectively. In addition $\Theta = (\theta_A, \theta_C, \theta_G, \theta_T)$.
\\\\
Similarly, the first order Markov chain case involves four independent multinomial processes, in which each process is preceded by one of the values. The predictive probability in this case is:

\begin {align*}
p (x| M_1) = &\int_{\Theta_A} \theta_{AA}^{n_{AA}} \theta_{AC}^{n_{AC}} \theta_{AG}^{n_{AG}} \theta_{AT}^{n_{AT}} d\Theta_A \times\\
& \int_{\Theta_C} \theta_{CA}^{n_{CA}} \theta_{CC}^{n_{CC}} \theta_{CG}^{n_{CG}} \theta_{CT}^{n_{CT}} d\Theta_C \times \\
& \int_{\Theta_G} \theta_{GA}^{n_{GA}} \theta_{GC}^{n_{GC}} \theta_{GG}^{n_{GG}} \theta_{GT}^{n_{GT}} d\Theta_G \times \\
& \int_{\Theta_T} \theta_{TA}^{n_{TA}} \theta_{TC}^{n_{GC}} \theta_{TG}^{n_{TG}} \theta_{TT}^{n_{TT}} d\Theta_T \\
\end {align*}

in which, $n_{i,j}$, where $i,j \in \{A,C,G,T\}$, denotes the number of occurrences of $(i,j)$ pairs in the sequence. Similarly, $\theta_{i,j}$ denotes the probability for $j$ to occur in case of being preceded by $i$. $\Theta_i$ denotes the parameter set $(\theta_{i,A}, \theta_{i,C}, \theta_{i,G}, \theta_{i,T})$.
\\\\
Similarly, the second order Markov chain is extended to 16 independent multinomial process. The predictive probability is:

\begin {equation}
  p (x| M_2) = \prod_{i,j \in \{A,C,G,T\}} \int_{\Theta_{ij}} \theta_{i,j,A}^{n_{i,j,A}} \theta_{i,j,C}^{n_{i,j,C}} \theta_{i,j,G}^{n_{i,j,G}} \theta_{i,j,T}^{n_{i,j,T}} d\Theta_{i,j} 
\end {equation}  

Assuming a Dirichlet distribution as the prior for the multinomial process, the integral has analytical result. 
\\\\
For the $M_0$ model, 
\begin {equation}
p (x| M_0) = \frac{\Gamma (\alpha_A + \alpha_C + \alpha_G + \alpha_T) \prod\limits_{m \in \{A,C,G,T\}} \Gamma (\alpha_m + n_m)} {\prod\limits_{m \in \{A,C,G,T\}} \Gamma (\alpha_m) \Gamma (\sum\limits_{m \in \{A,C,G,T\}}\alpha_m + \sum\limits_{m \in \{A,C,G,T\}}n_m)}
\end {equation}  

For the $M_1$ model

\begin {equation}
p (x| M_1) = \prod\limits_{i \in \{A,C,G,T\}} \frac{\Gamma (\alpha_{iA} + \alpha_{iC} + \alpha_{iG} + \alpha_iT) \prod\limits_{m \in \{A,C,G,T\}} \Gamma (\alpha_{im} + n_{im})} {\prod\limits_{m \in \{A,C,G,T\}} \Gamma (\alpha_{im}) \Gamma (\sum\limits_{m \in \{A,C,G,T\}}\alpha_{im} + \sum\limits_{m \in \{A,C,G,T\}}n_{im})}
\end {equation}  

For the $M_2$ model

\begin {equation}
p (x| M_1) = \prod\limits_{i,j \in \{A,C,G,T\}} \frac{\Gamma (\alpha_{ijA} + \alpha_{ijC} + \alpha_{ijG} + \alpha_ijT) \prod\limits_{m \in \{A,C,G,T\}} \Gamma (\alpha_{ijm} + n_{ijm})} {\prod\limits_{m \in \{A,C,G,T\}} \Gamma (\alpha_{ijm}) \Gamma (\sum\limits_{m \in \{A,C,G,T\}}\alpha_{ijm} + \sum\limits_{m \in \{A,C,G,T\}}n_{ijm})}
\end {equation}  

\subsection {Simulation Setup}

1000 sequences are generated, each of which is generated based on randomly generated state transition table, in which each entry in the table are uniformly distributed (and then normalised). In other word, two levels of randomness are involved. First level is associated with the random nature of the generation of gene sequence, which is essentially a Markov random process. Second level of randomness is  the randomness of underlying Markov state transition probability table. 


The purpose of the extra level of randomness is to enable us to investigate Bayes Factor in the general case.

\subsection {Pitfall of prior setup}
One thing to notice is the setting of the  prior parameter of the Dirichlet distribution. For the $M_0$ case, it is fine to set the prior as $Beta (1, 1, 1, 1)$ , however, for the $M_1$, it is tempting to set all four priors all be the same as $M_0$ case, $Beta (1, 1, 1, 1)$. However, there is one problem here in terms of both interpretation and theory. In terms of interpretation, the prior $Beta (1,1,1,1)$  resembles that we have some ``prior'' statistics on each possible value's occurrence frequency. Then unfairness arises. For the $M_0$, we have 4 occurrence's in total, while for $M_1$, with 4 priors, we have 16 such values, which receives more data than $M_0$. This is not reasonable. 

To make it fair, we should ensure that the sum of prior hyperparameters be equal for all models. If we set the sum to be 64 values. Then, for $M_0$, the prior hyperparameter would be $(16, 16, 16, 16)$, for $M_1$, it should have four independent Dirichlet, with hyperparameter $(4,4,4,4)$ and for$M_2$, 16 independent Dirichlet distributions are involved, each is hyperparameters by $(1,1,1,1)$.

\subsection {Simulation Result}

{\bf Bayes factor histograms}
\\\\
Using the 500 values in the sequence, 1000 sequences are generated, each of which is considered as an independent dataset and the corresponding  Bayes factor between ($M_0$, $M_1$) and ($M_1$, $M_2$) is calculated. The histogram are as followed. 
 
\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{bf-hist}
  \caption{Histogram of Bayes Factor of $M_{01}$(Above) and $M_{12}$(Below) calculated from 1000 sequences of length 500}
\end{figure}
  
We can see that Bayes Factor favours $M_2$ over $M_1$ and $M_1$ over $M_0$.

The mean value for $B_{01}$, $B_{12}$ is $0.0012$, $2.3740 \times 10^{-21}$. The model that Bayes factor supports is indeed the real generating model, $M_2$.
\\\\
{\bf Bayes factor vs sequence length}
\\\\
We consider the effect of sequence length, 100, 200, 300, 400 and 500 on Bayes factor respectively. The Bayes factor using each sequence is calculated and the averaged according to the underlying sequence length. For example, for length 100, 1000 sequences are generated thus 1000 Bayes factors for $M_{01}$ are calculated. To evaluate the overall Bayes factor value, the mean of 1000 Bayes factors is calculated.

The relationship between Bayes factor and the sequence length used in calculation is plotted as followed:

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{bf-nval}
  \caption{Bayes factor vs number of values: (above) $M_{01}$, (below) $M_{12}$}
\end{figure}

We can see an increasing trend of Bayes factor value for both $M_{01}$ and $M_{12}$ as the length increases. This is somewhat counter-intuitive, because as more data is involved in the calculation, the more sensible model should be preferred more.

\section{Problem 3}
Skip

%----------------------------------------------------------------------------------------
%	PROBLEM 4
%----------------------------------------------------------------------------------------

\section{Problem 4}

\subsection {Simulation setup}
In order to investigate the effect of $\varepsilon$. the value of $p$ in the transition probability matrix is first fixed and $\varepsilon$ is varied  from 0 to 1 at a step size of 0.05. For each $\varepsilon$ value, 500 sequences are generated randomly using the 1st order HMM parametrized by $A$ and $B$, which correspond to the state transition matrix and emission matrix respectively.

The initial state parameters are set to be equal, $[0.5, 0.5]$.

\subsection{Error measurement}

The error of the estimated parameter $A^{'}$ is defined as the mean of the element-wise absolute relative difference between $A$ and $A^{'}$.

\[ \mathrm{Error} (A^{'}, A) = \frac {1} {4} \sum_{i} \sum_{j} \frac{|A^{'}_{i,j} - A_{i,j}|}{A_{i,j}} \]

\subsection {Simulation result}

We consider three values of $p$, where $p \in \{0.2, 0.5, 0.8\}$. Using the above simulation setup for each $p$ value, the measured errors are given in the following three plots:

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{{noisyhmm-errorrate-p=0.200000}.png}
  \caption {ML parameter estimation error when $p$=0.2 in cases $\varepsilon$ from 0 to 1 at a step size $0.05$}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{{noisyhmm-errorrate-p=0.500000}.png}
  \caption {ML parameter estimation error when $p$=0.5 in cases $\varepsilon$ from 0 to 1 at a step size $0.05$}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{{noisyhmm-errorrate-p=0.800000}.png}
  \caption {ML parameter estimation error when $p$=0.8 in cases $\varepsilon$ from 0 to 1 at a step size $0.05$}
\end{figure}

\subsection {Oberservation \& Explanation}

There are two remarkable observations.

\begin{enumerate}
\item for $p=0.2$ and $p=0.8$, the error mean first increases. After reaching the peak at about 0.8, it starts to fall. 
\item while the plot for $p=0.2$ and $p=0.8$ are very similar, the plot for $p=0.5$ is not the case, it is flat.
\end {enumerate}

In the following, explanation is made to interprete the observation: 

\textbf{For the first observation:}\\

Denote the state set as $\{S_1, S_2\}$ and the observation set as $\{O_1, O_2\}$. 

For the first observation, it is intuitive to come up that lower error mean results from lower $\varepsilon$. Because the observations reflect the true state transition more or less accurately. 

However, it might be counter-intuitive for large $\varepsilon$ to have the similar pattern. Here are some explanation. 

First, we denote the count of certain observations as $C (O_i)$ or $C (O_i, O_j)$ for the count of $(O_i)$ and $(O_i, O_j)$ pairs respectively and the notation applies to state symbols similarly.

Let's assume the extreme case, $\varepsilon=1$, in which case each observation is flipped from its true underlying state. That is $S_1$ generates $O_2$ and $S_2$ generates $O_1$. Then $C (O_1, O_1) = C (S_2, S_2)$, $C (O_2, O_2) = C (S_1, S_1)$ , $C (O_2, O_1) = C (S_1, S_2)$ and $C (O_1, O_2) = C (S_2, S_1)$.

As there is only one parameter $p$ for the HMM state transition process, $C(S_1,S_1) $ and $ C(S_2,S_2)$ should be approximately equal if sample size is large enough. Similarly, $C(S_1, S_2) \approx C(S_2, S_1)$. Also as initial state distribution's entries are equal in this simulation setting, $C (S_1) \approx C (S_2)$.  As a result, $C (O_1, O_1) \approx C (O_2, O_2)$ and $C (O_2, O_1) \approx C (O_1, O_2)$ and $C (O_1) \approx C (O_2)$. 

As a illustration, we want to estimate the probability from $S_1$ to $S_2$:

\begin {align*}
  P (S_1, S_2) &= \frac {C (S_2)} {C (S_1, S_2)} \\
  &= \frac {C (O_1)} {C (O_2, O_1)} \\
  &\approx \frac {C (O_2)} {C (O_1, O_2)}\\
  &=P (O_1, O_2)
\end {align*}

In which, $P (O_1, O_2)$ is our estimated probability from $S_2$ to $S_1$, by using the observations and  $P (S_1, S_2)$ is the estimated result by using the underlying states, which is closer to the real parameter. As they are approximately equal, the error mean for large $\varepsilon$ is as low as that of small $\varepsilon$.

\textbf{For the second observation:} \\

When $p=0.5$, the state transition process is like tossing a fair coin again and again.

The low error mean for both small and great $p$ can be explained like above. In case $\varepsilon=0.5$, flipping of the observation is like another process of tossing fair coin.

Though certain observations may be flipped, the cases in which $S_1$ emits $O_2$ are compensated by the fact that roughly the same amount of emission from $S_2$ to $O_1$, thus the sufficient statistics does not change much.

As a conclusion, the error mean for $p=0.5$ does not vary much from that of large or small $p$ values.

%----------------------------------------------------------------------------------------
%	PROBLEM 5
%----------------------------------------------------------------------------------------
\section{Problem 5}
\subsection {Choice of HMM parameter}

The state transition probability matrix is set to:

\begin{table}[H]
\caption{State transition probability matrix: column labels represent source states and row labels represent target states}
\centering
\begin{tabular}{c|ccc}
  & CpG rich & CpG poor  \\ \hline
  CpG rich & 0.99 & 0.01 \\
  CpG poor & 0.01 & 0.99
\end {tabular}
\end {table}


The emission probability matrix is set to:

\begin{table}[H]
\caption{Emission probability matrix}
\centering
\begin{tabular}{c|cccc}
  & A & C & G & T  \\ \hline
  CpG rich & 0.05 & 0.49 & 0.49 & 0.05 \\
  CpG poor & 0.49 & 0.05 & 0.05 & 0.49
\end {tabular}
\end {table}

In this way, we can ensure that state is very unlikely to transfer to its  counterpart and CpG rich state is much likely to produces $C$ and $G$, while CpG poor state is much likely to produces $A$ and $T$
\subsection {Data generation \& Visualization}

100 sequences each of length 100 using the previously stated model are randomly generated. To achieve more natural visual contrast between the state sequence and emission sequence, the emitted symbol set are transformed in the following way, $G$ and $C$ are combined into one symbol, CpG rich, while $A$ and $T$ are combined into CpG poor. Thus, emission sequences are converted into the same type as state sequence, which involves only two states.

The visualization for both sequences is as followed:

\begin{figure}[H]
  \center
  \includegraphics[scale=.8]{gene_viz}
  \label{fig:5.gene_viz}
  \caption {Visualization for 100 transformed emission sequences(left) and the underlying state sequences(right).}
\end{figure}

As we can see, state sequences are ``tidier'' than the emission sequence in the sense that there are fewer times of state flipping.

\subsection {Modeling as ordinary Markov chain}

In case of modeling this process using ordinary first order Markov chain, the state space becomes $\{{\text{CpG rich}, \text{CpG poor}}\}$, the problem of parameter estimation boils down to estimate the state transition probabilities. ML estimation is chosen as the  parameter estimation method. 

Before estimation, we reassign the emission symbols in the following way, $C$ and $G$ are treated as CpG rich and $A$ and $T$ are treated as CpG poor. This is done to fit the data into this ordinary Markov model. No smoothing technique is used here.

The estimated state transition probabilities are given below:

\begin{table}[H]
\caption{Estimated state transition probability matrix using the 1st order Markov model }
\centering
\begin{tabular}{c|ccc}
  & CpG rich & CpG poor  \\ \hline
  CpG rich & 0.8669 &   0.1331 \\
  CpG poor & 0.2678 &   0.7322
\end {tabular}
\end {table}

As we can see, compared to the generating parameter, the estimated result is ``smoothed out'' to some extent, in the sense that the probabilities are centered towards 0.5, compared to the true values. For example, probability from CpG rich to CpG rich is 0.8669, which closer to 0.5 than the true value 0.99. The same applies to the probability from CpG rich to CpG poor, 0.1331, compared to the true value 0.01.

The cause for this phenomenon is due to the randomness in the emission process. There is some possibility for CpG rich state to produce symbols $A$ and $T$, though such probability is low. One direct consequence is the interspersing contrasting subsequences (e.g, the little green spots in the left image in the Figure ~\ref{fig:5.gene_viz}) in the emission sequences. Those contrasting short subsequences increases occurrences of CpG rich transiting to CpG poor and CpG poor transiting to CpG rich. Thus, they serve as the ``bridge'' that connects the underlying two states and make them seem to transit to each other more often if the ordinary Markov model is considered.

%----------------------------------------------------------------------------------------
%	PROBLEM 6
%----------------------------------------------------------------------------------------
\section {Problem 6}
\subsection {Data preparation}
First order Markov chain (Markov (1)) is chosen as the models to be studied in this problem, as relevant utility Matlab packages can be used directly.

In order to investigate the effect of sequence length, we consider two types of sequence length change pattern, exponential increase, $1, 2, 4, \cdots, 256$ and linear increase, $100, 200, \cdots, 1000$, in order to investigate the effect in more details. Rather than predefine the modal parameters, the parameters are generated randomly. If we define one iteration of simulation as the process of generating one sequence of each sequence length respectively, then in one such iteration, the model parameters are the same, while it does not hold true for two different iterations.

In order to mitigate the effect of randomness in model parameter and study the effect in the general case, the process is performed 50 times, each with independently generated model parameters.

\subsection {Estimation setup}

For each iteration of model parameter estimation, two parameter estimation algorithms, Baum-Welch algorithm and Viterbi algorithm are used. In other words, we estimate the parameter twice for a given model using the two algorithms.

For both algorithms, it is required to give an initial guess for the model parameter for the algorithms to start with. As the initial guess can have a great influence on the final result, the initial guess to set the be not far from the real model parameter, for example, for each probability entry, it difference to the real value is within $[-0.05, 0.05]$. We make this choice because we are only interested in the effect of sequence length on estimated model parameter in this problem, huge variation in estimation result due to the initial guessing is not desirable. So  giving a good starting point for the estimation algorithm helps the algorithms converge as close as possible to the real parameters, thus swiping off irrelevant factors and enabling us to focus sequence length.

To avoid the avoid zero probability estimates for emissions in Viterbi learning, which is more likely t happen for short sequence, we specify the pseudo-count emission values for all entries to be 1.

\subsection {Error measurement}
In each iteration of model estimation, to measure the error of estimated parameters, average squared error is used here. In this case, it is defined as followed:

\[ E () = 1 / (M N + N P) (\sum\limits_{i,j}^{M,N} (A_{i,j} - A^{'}_{i,j})^{2} + \sum\limits_{i^{'},j^{'}}^{N,P} (B_{i^{'},j^{'}} - B^{'}_{i^{'},j^{'}})^{2})\]

in which, $A$, an M by N matrix and $B$, an N by P matrix, are the generating  state transition parameters and the emission parameters respectively.  $A^{'}$  and $B^{'}$ are the corresponding estimation.

The final error made in each sequence length setting is simply the average of the average square error from all iterations of parameter estimation.

\subsection {Effect of sequence length}

After running the all simulation iterations, error rate plot for both the linear increasing case and exponential increasing case are given as followed:

\begin{figure}[H]
  \centering
  \includegraphics[scale=.8]{err_vs_seqlen}
  \caption {Average of mean square error rate vs sequence length: (Left) sequence length changes exponentially from 1 to 512 by power of 2. (Right) sequence length changes linearly from 100 to 1000 by step size of 100.}
  \label{fig:6.err_vs_seqlen}
\end{figure}

\subsection {Discussion on sequence length effect}

As we can see in Figure ~\ref{fig:6.err_vs_seqlen}, the effect of sequence length differs in terms of different estimation algorithms. 

In the case of Baum Welch algorithm, the estimation error drops remarkably as the sequence length increases, while the situation for Viterbi algorithm is quite counter-intuitive. Despite a slight drop at the beginning in the left figure, the error does not change as greatly that much compared to Baum Welch algorithm. Although Baum Welch algorithms generally performs better for longer sequence (length larger than roughly 16 in this experiment), Viterbi algorithm performs better for shorter sequences (length $\le$ 16 roughly), as we can see in the following figure, and is more stable than Baum Welch algorithm.

The seemingly stability of Viterbi algorithm reflects the relative low accuracy compared to Baum Welch algorithm. This is closely related to the nature of Viterbi algorithm, which saves running time in sacrifice of estimation accuracy. This explains why Viterbi algorithm performs generally worse than Baum Welch algorithm. However, the explanation of stability is out of the scope of this study.

In terms of how length increasing pattern affects parameter estimation, Viterbi algorithm is not so typical as it does not change much. While for Baum Welch algorithm, the error rate drops in a nearly linear fashion if the length increases linearly while drops exponentially if the length increase exponentially. This reflects the fact that Baum Welch algorithm's estimation accuracy is closely related to the sequence length. It is even possible to fit a curve in terms of how accuracy changes with sequence length and predict such accuracy given new sequence length.

\end{document}
